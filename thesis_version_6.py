# -*- coding: utf-8 -*-
"""thesis version 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KDYPE0MaqlM5-vU6esFgh_a1u1wA9AqB
"""

# =========================
# 0) CLEAN INSTALL (RUN ONCE) + RESTART
# =========================
# If you're on Colab: Runtime -> Restart runtime after this cell (we auto-kill too)

!pip -q uninstall -y torch torchvision torchaudio torch_geometric pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv || true

# --- Fix scientific stack to consistent versions (avoid ABI break + satisfy common deps) ---
!pip -q install --no-cache-dir --force-reinstall \
  "numpy==2.0.2" \
  "scipy==1.14.1" \
  "scikit-learn==1.6.1" \
  "pandas==2.2.2" \
  "matplotlib==3.9.2" \
  "networkx==3.4.2" \
  "pillow==11.3.0"

# --- Install PyTorch CUDA 12.6 build (choose cu126 to match your runtime CUDA 12.6) ---
# If this fails in your environment, tell me the exact error text.
!pip -q install --index-url https://download.pytorch.org/whl/cu126 \
  "torch==2.8.0+cu126" "torchvision==0.23.0+cu126" "torchaudio==2.8.0+cu126"

# --- Install PyG + extensions matching torch/cuda ---
import torch, sys
TORCH_VER = torch.__version__.split('+')[0]  # e.g. 2.8.0
CUDA_VER  = torch.version.cuda              # e.g. 12.6
CUDA_TAG  = f"cu{CUDA_VER.replace('.','')}" if CUDA_VER else "cpu"
print("python:", sys.version)
print("torch:", torch.__version__)
print("cuda:", CUDA_VER, "| tag:", CUDA_TAG)

!pip -q install -U torch_geometric
!pip -q install -U pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \
  -f https://data.pyg.org/whl/torch-{TORCH_VER}+{CUDA_TAG}.html

# --- clustering + GO enrichment ---
!pip -q install -U python-louvain gprofiler-official
!pip -q install -U igraph leidenalg || true  # optional (if fails, we fallback)

# --- hard restart (important after ABI changes) ---
import os, signal
os.kill(os.getpid(), signal.SIGKILL)

# =========================
# 1) IMPORTS + SEEDS + DEVICE
# =========================
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import torch
import torch.nn.functional as F

from torch_geometric.data import Data
from torch_geometric.transforms import RandomLinkSplit

from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

SEED = 0
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

# quick ABI sanity:
import scipy, sklearn
print("numpy:", np.__version__)
print("scipy:", scipy.__version__)
print("sklearn:", sklearn.__version__)
print("torch:", torch.__version__)
print("pyg ok")

# =========================
# 2) DOWNLOAD STRING (v11.5) + UNZIP
# =========================
import pathlib, gzip, shutil

data_dir = pathlib.Path("data")
data_dir.mkdir(exist_ok=True)

def maybe_download(url, out_path):
    out_path = pathlib.Path(out_path)
    if out_path.exists():
        print("OK (exists):", out_path)
        return
    print("Downloading:", out_path.name)
    !wget -q -O "{out_path}" "{url}"
    print("Saved:", out_path)

maybe_download(
    "https://stringdb-static.org/download/protein.info.v11.5/9606.protein.info.v11.5.txt.gz",
    data_dir / "9606.protein.info.v11.5.txt.gz"
)
maybe_download(
    "https://stringdb-static.org/download/protein.links.detailed.v11.5/9606.protein.links.detailed.v11.5.txt.gz",
    data_dir / "9606.protein.links.detailed.v11.5.txt.gz"
)

for gz in data_dir.glob("*.gz"):
    out = gz.with_suffix("")
    if not out.exists():
        print("Unzipping:", gz.name)
        with gzip.open(gz, "rb") as f_in, open(out, "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)
        print("OK:", out)

# =========================
# 3) LOAD STRING GRAPH -> PyG Data + nodes_annot
# =========================
info = pd.read_csv("data/9606.protein.info.v11.5.txt", sep="\t")
info = info.rename(columns={"#string_protein_id": "string_id"})

links = pd.read_csv("data/9606.protein.links.detailed.v11.5.txt", sep=" ")
if "combined_score" in links.columns:
    links["w"] = links["combined_score"].astype(float) / 1000.0
else:
    score_col = [c for c in links.columns if c not in ["protein1", "protein2"]][0]
    links["w"] = links[score_col].astype(float)
    links["w"] = (links["w"] - links["w"].min()) / (links["w"].max() - links["w"].min() + 1e-9)

ids = pd.unique(links[["protein1", "protein2"]].values.ravel("K"))
string_ids = ids.astype(str).tolist()
id2idx = {sid: i for i, sid in enumerate(string_ids)}

u = links["protein1"].map(id2idx).to_numpy()
v = links["protein2"].map(id2idx).to_numpy()
w = links["w"].to_numpy().astype(np.float32)

# unique undirected edges, keep max weight
a = np.minimum(u, v); b = np.maximum(u, v)
df_e = pd.DataFrame({"a": a, "b": b, "w": w}).groupby(["a","b"], as_index=False)["w"].max()

edge_index = torch.tensor(df_e[["a","b"]].to_numpy().T, dtype=torch.long)
edge_weight = torch.tensor(df_e["w"].to_numpy(), dtype=torch.float32)

w_dict = {(int(r.a), int(r.b)): float(r.w) for r in df_e.itertuples(index=False)}

nodes_annot = pd.DataFrame({"node": np.arange(len(string_ids)), "string_id": string_ids}).merge(
    info[["string_id", "preferred_name", "annotation"]],
    on="string_id",
    how="left",
)
nodes_annot["gene_final"] = nodes_annot["preferred_name"]

# simple feature: degree (you can later add clustering coef, k-core, pagerank etc)
num_nodes = len(string_ids)
deg = np.zeros((num_nodes, 1), dtype=np.float32)
deg[df_e["a"].to_numpy()] += 1
deg[df_e["b"].to_numpy()] += 1
x = torch.tensor(deg, dtype=torch.float32)

data_full = Data(x=x, edge_index=edge_index, edge_weight=edge_weight, num_nodes=num_nodes)
print("data_full:", data_full)
display(nodes_annot.head())

# =========================
# 4) RandomLinkSplit + edge_weight recovery
# =========================
data_lp = Data(x=data_full.x, edge_index=data_full.edge_index, edge_weight=data_full.edge_weight, num_nodes=data_full.num_nodes)

split = RandomLinkSplit(
    num_val=0.10,
    num_test=0.10,
    is_undirected=True,
    add_negative_train_samples=True,
    neg_sampling_ratio=1.0,
)
data_train, data_val, data_test = split(data_lp)

print("Train graph edges:", data_train.edge_index.size(1))
print("Val keys:", list(data_val.keys()))
print("Test keys:", list(data_test.keys()))

def edge_weight_from_dict(edge_index, w_dict):
    ei = edge_index.detach().cpu().numpy()
    ws = np.zeros(ei.shape[1], dtype=np.float32)
    for i in range(ei.shape[1]):
        u, v = int(ei[0, i]), int(ei[1, i])
        a, b = (u, v) if u < v else (v, u)
        ws[i] = w_dict.get((a, b), 1.0)
    return torch.tensor(ws, dtype=torch.float32)

data_train.edge_weight = edge_weight_from_dict(data_train.edge_index, w_dict)
data_val.edge_weight   = edge_weight_from_dict(data_val.edge_index, w_dict)
data_test.edge_weight  = edge_weight_from_dict(data_test.edge_index, w_dict)

# =========================
# 5) Robust edge label getters
# =========================
def has_edge_label_format(d):
    return ("edge_label_index" in d.keys()) and ("edge_label" in d.keys())

def has_posneg_format(d):
    return ("pos_edge_label_index" in d.keys()) and ("neg_edge_label_index" in d.keys())

def get_eval_edges(data_split):
    if has_edge_label_format(data_split):
        eidx = data_split.edge_label_index
        y = data_split.edge_label.detach().cpu().numpy().astype(int)
        return eidx, y
    if has_posneg_format(data_split):
        pos = data_split.pos_edge_label_index
        neg = data_split.neg_edge_label_index
        eidx = torch.cat([pos, neg], dim=1)
        y = np.concatenate([np.ones(pos.size(1), dtype=int), np.zeros(neg.size(1), dtype=int)])
        return eidx, y
    raise RuntimeError(f"Unknown split format. Keys: {data_split.keys()}")

def get_train_edges_and_labels(data_train):
    if has_edge_label_format(data_train):
        return data_train.edge_label_index, data_train.edge_label.float()
    if has_posneg_format(data_train):
        pos = data_train.pos_edge_label_index
        neg = data_train.neg_edge_label_index
        eidx = torch.cat([pos, neg], dim=1)
        y = torch.cat([torch.ones(pos.size(1)), torch.zeros(neg.size(1))], dim=0).float()
        return eidx, y
    raise RuntimeError(f"Unknown train format. Keys: {data_train.keys()}")

def eval_scores_to_metrics(y_true, y_score):
    return roc_auc_score(y_true, y_score), average_precision_score(y_true, y_score)

_, yv = get_eval_edges(data_val)
_, yt = get_eval_edges(data_test)
print("Val pos/neg:", int(yv.sum()), int((yv == 0).sum()))
print("Test pos/neg:", int(yt.sum()), int((yt == 0).sum()))

# =========================
# 6) Baselines on TRAIN graph
# =========================
Gtr = nx.Graph()
ei = data_train.edge_index.detach().cpu().numpy()
Gtr.add_edges_from(list(zip(ei[0], ei[1])))

adj = {u: set(Gtr.neighbors(u)) for u in Gtr.nodes()}
deg = {u: len(adj[u]) for u in adj}

def common_neighbors_score(u, v):
    if u not in adj or v not in adj:
        return 0.0
    return float(len(adj[u].intersection(adj[v])))

def adamic_adar_score(u, v):
    if u not in adj or v not in adj:
        return 0.0
    cn = adj[u].intersection(adj[v])
    s = 0.0
    for w in cn:
        dw = deg.get(w, 0)
        if dw > 1:
            s += 1.0 / np.log(dw)
    return float(s)

def baseline_scores_for_split(data_split, scorer_fn):
    eidx, y = get_eval_edges(data_split)
    e = eidx.detach().cpu().numpy()
    scores = np.zeros(e.shape[1], dtype=np.float32)
    for i in range(e.shape[1]):
        u, v = int(e[0, i]), int(e[1, i])
        scores[i] = scorer_fn(u, v)
    return y, scores

y_test, s_cn = baseline_scores_for_split(data_test, common_neighbors_score)
roc_cn, ap_cn = eval_scores_to_metrics(y_test, s_cn)

y_test, s_aa = baseline_scores_for_split(data_test, adamic_adar_score)
roc_aa, ap_aa = eval_scores_to_metrics(y_test, s_aa)

print(f"[CN]        ROC-AUC={roc_cn:.4f}  PR-AUC={ap_cn:.4f}")
print(f"[AdamicAA]  ROC-AUC={roc_aa:.4f}  PR-AUC={ap_aa:.4f}")

# =========================
# 7) APPNP (GNN) link prediction model
# =========================
from torch_geometric.nn import APPNP

class APPNPEncoder(torch.nn.Module):
    def __init__(self, in_dim, hidden=128, out_dim=64, K=10, alpha=0.1, dropout=0.5):
        super().__init__()
        self.lin1 = torch.nn.Linear(in_dim, hidden)
        self.lin2 = torch.nn.Linear(hidden, out_dim)
        self.prop = APPNP(K=K, alpha=alpha, dropout=dropout)

    def forward(self, x, edge_index, edge_weight=None):
        x = F.dropout(x, p=0.2, training=self.training)
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.2, training=self.training)
        x = self.lin2(x)
        # version-robust edge_weight passing
        try:
            x = self.prop(x, edge_index, edge_weight=edge_weight)
        except TypeError:
            x = self.prop(x, edge_index)
        return x

class LinkDecoder(torch.nn.Module):
    def __init__(self, emb_dim=64, hidden=64):
        super().__init__()
        self.lin1 = torch.nn.Linear(emb_dim, hidden)
        self.lin2 = torch.nn.Linear(hidden, 1)

    def forward(self, z, edge_index):
        h = z[edge_index[0]] * z[edge_index[1]]
        h = F.relu(self.lin1(h))
        return self.lin2(h).view(-1)

class LinkPredModel(torch.nn.Module):
    def __init__(self, in_dim, hidden=128, emb_dim=64, K=10, alpha=0.1, dropout=0.5):
        super().__init__()
        self.enc = APPNPEncoder(in_dim, hidden=hidden, out_dim=emb_dim, K=K, alpha=alpha, dropout=dropout)
        self.dec = LinkDecoder(emb_dim=emb_dim, hidden=emb_dim)

    def forward(self, x, edge_index, edge_weight, edge_label_index):
        z = self.enc(x, edge_index, edge_weight)
        logits = self.dec(z, edge_label_index)
        return logits, z

def train_linkpred(model, data_train, data_val, epochs=150, lr=1e-3, wd=1e-5):
    model = model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)

    train_log = {"epoch": [], "loss": [], "val_roc": [], "val_ap": []}

    def eval_split(data_split):
        model.eval()
        with torch.no_grad():
            ew = getattr(data_split, "edge_weight", None)
            ew = ew.to(device) if ew is not None else None
            eidx, y = get_eval_edges(data_split)
            logits, _ = model(
                data_split.x.to(device),
                data_split.edge_index.to(device),
                ew,
                eidx.to(device),
            )
            p = torch.sigmoid(logits).detach().cpu().numpy()
            roc, ap = eval_scores_to_metrics(y, p)
        return roc, ap, y, p

    best_ap = -1.0
    best_state = None

    for ep in range(1, epochs + 1):
        model.train()
        opt.zero_grad()

        eidx, y = get_train_edges_and_labels(data_train)
        logits, _ = model(
            data_train.x.to(device),
            data_train.edge_index.to(device),
            data_train.edge_weight.to(device),
            eidx.to(device),
        )
        loss = F.binary_cross_entropy_with_logits(logits, y.to(device))
        loss.backward()
        opt.step()

        if ep % 10 == 0 or ep == 1:
            roc_v, ap_v, _, _ = eval_split(data_val)
            print(f"[APPNP] ep {ep:03d} | loss={loss.item():.4f} | val ROC={roc_v:.4f} | val AP={ap_v:.4f}")
            train_log["epoch"].append(ep)
            train_log["loss"].append(float(loss.item()))
            train_log["val_roc"].append(float(roc_v))
            train_log["val_ap"].append(float(ap_v))

            if ap_v > best_ap:
                best_ap = ap_v
                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}

    if best_state is not None:
        model.load_state_dict(best_state)

    roc_t, ap_t, y_t, p_t = eval_split(data_test)
    print(f"[APPNP TEST] ROC-AUC={roc_t:.4f} PR-AUC={ap_t:.4f}")
    return model, (y_t, p_t), train_log

model = LinkPredModel(data_train.num_node_features, hidden=128, emb_dim=64, K=10, alpha=0.1, dropout=0.5)
model, (y_appnp, p_appnp), train_log = train_linkpred(model, data_train, data_val, epochs=150)

# =========================
# 8) PAPER PLOTS
# =========================
def plot_pr_roc(y_true, score_dict, title_prefix=""):
    plt.figure()
    for name, s in score_dict.items():
        prec, rec, _ = precision_recall_curve(y_true, s)
        plt.plot(rec, prec, label=name)
    plt.xlabel("Recall"); plt.ylabel("Precision")
    plt.title(f"{title_prefix} Precision-Recall")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("pr_curve.png", dpi=300)
    plt.show()

    plt.figure()
    for name, s in score_dict.items():
        fpr, tpr, _ = roc_curve(y_true, s)
        plt.plot(fpr, tpr, label=name)
    plt.xlabel("FPR"); plt.ylabel("TPR")
    plt.title(f"{title_prefix} ROC")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("roc_curve.png", dpi=300)
    plt.show()

score_dict = {"CN": s_cn, "Adamic-Adar": s_aa, "APPNP (GNN)": p_appnp}
plot_pr_roc(y_appnp, score_dict, title_prefix="TEST:")

rows = [
    ("CN", roc_cn, ap_cn),
    ("Adamic-Adar", roc_aa, ap_aa),
    ("APPNP (GNN)", roc_auc_score(y_appnp, p_appnp), average_precision_score(y_appnp, p_appnp)),
]
df_lp = pd.DataFrame(rows, columns=["method", "ROC_AUC", "PR_AUC"])
display(df_lp)

plt.figure(figsize=(7,4))
plt.bar(df_lp["method"], df_lp["PR_AUC"])
plt.xticks(rotation=20, ha="right")
plt.ylabel("PR-AUC (TEST)")
plt.title("Link Prediction: PR-AUC vs baselines")
plt.tight_layout()
plt.savefig("pr_auc_barplot.png", dpi=300)
plt.show()

df_lp.to_csv("linkpred_baselines_vs_gnn.csv", index=False)
print("Saved: linkpred_baselines_vs_gnn.csv")

# training curves
plt.figure()
plt.plot(train_log["epoch"], train_log["loss"])
plt.xlabel("Epoch"); plt.ylabel("Train loss")
plt.title("Training loss (logged)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("train_loss_curve.png", dpi=300)
plt.show()

plt.figure()
plt.plot(train_log["epoch"], train_log["val_ap"])
plt.xlabel("Epoch"); plt.ylabel("Val AP")
plt.title("Validation AP (logged)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("val_ap_curve.png", dpi=300)
plt.show()

# =========================
# 9) TOP-N STRICT (NO LEAKAGE) — uses TRAIN GRAPH embeddings only
#     Solution A: all tensors on same device
# =========================
model.eval()
with torch.no_grad():
    z_train = model.enc(
        data_train.x.to(device),
        data_train.edge_index.to(device),
        data_train.edge_weight.to(device)
    )

# existing edges in TRAIN graph (undirected)
Eset_train = set()
ei_tr = data_train.edge_index.detach().cpu().numpy()
for u, v in zip(ei_tr[0], ei_tr[1]):
    a, b = int(u), int(v)
    if a > b: a, b = b, a
    Eset_train.add((a, b))

# candidate sampling
rng = np.random.default_rng(SEED)
N = int(data_train.num_nodes)
NUM_CAND = 300000
pairs = rng.integers(0, N, size=(NUM_CAND, 2), dtype=np.int64)
pairs = pairs[pairs[:,0] != pairs[:,1]]

cand = []
for u, v in pairs:
    a, b = int(u), int(v)
    if a > b: a, b = b, a
    if (a, b) not in Eset_train:
        cand.append((a, b))
    if len(cand) >= 200000:
        break

cand = np.array(cand, dtype=np.int64)
edge_cand = torch.tensor(cand.T, dtype=torch.long, device=device)

with torch.no_grad():
    logits = model.dec(z_train, edge_cand).detach().cpu().numpy()
prob = 1.0 / (1.0 + np.exp(-logits))

topk = 200
idx = np.argsort(-prob)[:topk]
top_edges = cand[idx]
top_prob  = prob[idx]

def cn_explain(u, v):
    nu = adj.get(int(u), set())
    nv = adj.get(int(v), set())
    return len(nu.intersection(nv))

out = []
for (u, v), p in zip(top_edges, top_prob):
    out.append({
        "u": int(u), "v": int(v),
        "string_u": string_ids[int(u)],
        "string_v": string_ids[int(v)],
        "prob": float(p),
        "cn_train": int(cn_explain(u, v)),
        "gene_u": nodes_annot.loc[int(u), "gene_final"],
        "gene_v": nodes_annot.loc[int(v), "gene_final"],
    })

df_top = pd.DataFrame(out).sort_values("prob", ascending=False)
df_top.to_csv("top_predictions_strict_train_graph.csv", index=False)
print("Saved: top_predictions_strict_train_graph.csv")
display(df_top.head(10))

# =========================
# 10) OPTIONAL: RETRAIN ON FULL GRAPH (deliverable mode)
#    Robust: handles whatever train_linkpred returns (model, (y,p)) OR (model, history, (y,p)) etc.
# =========================
DO_RETRAIN_ALL = True

import numpy as np

def _to_1d_array(x):
    try:
        a = np.asarray(x)
    except Exception:
        return None
    if a.ndim != 1 or a.size == 0:
        return None
    return a

def extract_model_and_yp(out):
    """
    Returns: model, y, p, history (history may be None)
    Works with returns like:
      (model, (y,p))
      (model, y, p)
      (model, history_dict, (y,p))
      (model, (y,p), history_dict)
      (model, something_else, ...)
    """
    model = None
    y = None
    p = None
    history = None

    if not isinstance(out, tuple):
        return out, None, None, None

    # first element is almost always model
    model = out[0]

    # scan all other elements to find (y,p) pair and/or history
    for item in out[1:]:
        # history often dict with epoch/loss keys
        if isinstance(item, dict) and ("epoch" in item or "loss" in item or "val_roc" in item):
            history = item
            continue

        # the "good" case: a tuple (y,p)
        if isinstance(item, tuple) and len(item) == 2:
            y1 = _to_1d_array(item[0])
            p1 = _to_1d_array(item[1])
            if y1 is not None and p1 is not None and y1.shape[0] == p1.shape[0]:
                y, p = y1, p1
                continue

    # fallback: maybe out = (model, y, p)
    if y is None and len(out) >= 3:
        y1 = _to_1d_array(out[1])
        p1 = _to_1d_array(out[2])
        if y1 is not None and p1 is not None and y1.shape[0] == p1.shape[0]:
            y, p = y1, p1

    return model, y, p, history


if DO_RETRAIN_ALL:
    data_all = Data(
        x=data_full.x,
        edge_index=data_full.edge_index,
        edge_weight=data_full.edge_weight,
        num_nodes=data_full.num_nodes
    )

    split2 = RandomLinkSplit(
        num_val=0.0,
        num_test=0.0,
        is_undirected=True,
        add_negative_train_samples=True,
        neg_sampling_ratio=1.0
    )
    tr_all, va_all, te_all = split2(data_all)

    # weights (from your dict)
    tr_all.edge_weight = edge_weight_from_dict(tr_all.edge_index, w_dict)

    # ✅ crucial fix: dummy val = train so eval doesn't crash on empty arrays
    va_all = tr_all

    model_all = LinkPredModel(
        tr_all.num_node_features,
        hidden=128,
        emb_dim=64,
        K=10,
        alpha=0.1,
        dropout=0.5
    )

    out = train_linkpred(model_all, tr_all, va_all, epochs=100)
    model_all, y_all, p_all, hist_all = extract_model_and_yp(out)

    print("✅ Retrain-on-all done. Use model_all for final deliverable predictions.")

    if y_all is not None and p_all is not None:
        y_all = np.asarray(y_all).astype(int)
        p_all = np.asarray(p_all).astype(float)
        print("Sanity metrics (val=train):",
              "ROC-AUC=", roc_auc_score(y_all, p_all),
              "PR-AUC=", average_precision_score(y_all, p_all))
    else:
        print("⚠️ Could not extract (y,p) from train_linkpred output. "
              "That's OK—training finished. We'll just skip sanity metrics.")

!pip -q uninstall -y community
!pip -q install python-louvain

# =========================
# 11) CLUSTERING ON EMBEDDINGS + METRICS + PLOTS
# FIX: modularity needs a true partition of G nodes.
#      We build a graph with ALL nodes [0..N-1] and the TRAIN edges.
# =========================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import community.community_louvain as community_louvain

from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from networkx.algorithms.community.quality import modularity as nx_modularity

# --- embeddings (STRICT: from train graph) ---
Z = z_train.detach().cpu().numpy()
N = int(data_train.num_nodes)

# --- Build Gtr_allnodes: same train edges, but ensure all nodes exist ---
Gtr_all = nx.Graph()
Gtr_all.add_nodes_from(range(N))  # IMPORTANT: include isolates too
ei = data_train.edge_index.detach().cpu().numpy()
Gtr_all.add_edges_from(list(zip(ei[0], ei[1])))

def compute_conductance(G, clusters):
    degG = dict(G.degree())
    neigh = {u: set(G.neighbors(u)) for u in G.nodes()}
    all_nodes = set(G.nodes())

    def vol(S):
        return sum(degG[u] for u in S)

    conds = []
    for C in clusters:
        C = set(C)
        if len(C) == 0 or len(C) == len(all_nodes):
            continue
        # cut edges leaving C
        cut = 0
        for u in C:
            cut += len(neigh[u] - C)
        vC = vol(C)
        vNot = vol(all_nodes - C)
        denom = max(1e-9, min(vC, vNot))
        conds.append(cut / denom)
    return float(np.mean(conds)) if conds else np.nan

# ---------- KMeans on embeddings ----------
K = 20
kmeans = KMeans(n_clusters=K, n_init=10, random_state=SEED)
k_labels = kmeans.fit_predict(Z)

# clusters are a full partition of [0..N-1]
clusters_km = [np.where(k_labels == c)[0].tolist() for c in range(K)]

# ✅ modularity on graph that contains ALL nodes
mod_km = nx_modularity(Gtr_all, clusters_km)
cond_km = compute_conductance(Gtr_all, clusters_km)
print(f"[KMeans] K={K} | modularity(train)={mod_km:.4f} | avg conductance={cond_km:.4f}")

# ---------- Louvain on TRAIN graph (all nodes) ----------
import community.community_louvain as community_louvain
part = community_louvain.best_partition(Gtr_all, random_state=SEED)

comm_ids = sorted(set(part.values()))
clusters_lv = [[n for n, c in part.items() if c == cid] for cid in comm_ids]

mod_lv = nx_modularity(Gtr_all, clusters_lv)
cond_lv = compute_conductance(Gtr_all, clusters_lv)
print(f"[Louvain] communities={len(clusters_lv)} | modularity(train)={mod_lv:.4f} | avg conductance={cond_lv:.4f}")

# ---------- Leiden (optional) ----------
leiden_ok = False
try:
    import igraph as ig
    import leidenalg
    leiden_ok = True
except Exception:
    leiden_ok = False

if leiden_ok:
    edges = list(Gtr_all.edges())
    g = ig.Graph(n=N, edges=edges, directed=False)  # n=N ensures all nodes exist
    part_ld = leidenalg.find_partition(g, leidenalg.RBConfigurationVertexPartition, seed=SEED)
    clusters_ld = [list(c) for c in part_ld]

    mod_ld = nx_modularity(Gtr_all, clusters_ld)
    cond_ld = compute_conductance(Gtr_all, clusters_ld)
    print(f"[Leiden] communities={len(clusters_ld)} | modularity(train)={mod_ld:.4f} | avg conductance={cond_ld:.4f}")

# ---------- Paper plots ----------
tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, init="pca")
Z2 = tsne.fit_transform(Z)

plt.figure(figsize=(7,6))
plt.scatter(Z2[:, 0], Z2[:, 1], c=k_labels, s=6)
plt.title("t-SNE of APPNP embeddings (colored by KMeans clusters)")
plt.xlabel("t-SNE-1"); plt.ylabel("t-SNE-2")
plt.tight_layout()
plt.savefig("tsne_embeddings_kmeans.png", dpi=300)
plt.show()

sizes = [len(c) for c in clusters_km]
plt.figure(figsize=(7,4))
plt.bar(np.arange(K), sizes)
plt.xlabel("Cluster id (KMeans)")
plt.ylabel("Size (#nodes)")
plt.title("Cluster sizes (KMeans on embeddings)")
plt.tight_layout()
plt.savefig("cluster_sizes_kmeans.png", dpi=300)
plt.show()

# ---------- Save cluster assignments ----------
df_clusters = pd.DataFrame({"node": np.arange(N), "kmeans_cluster": k_labels})
if "nodes_annot" in globals():
    df_clusters = df_clusters.merge(nodes_annot[["node","string_id","gene_final"]], on="node", how="left")
df_clusters.to_csv("embedding_clusters_kmeans.csv", index=False)
print("Saved: embedding_clusters_kmeans.csv")

# =========================
# 12) GO ENRICHMENT (g:Profiler)
# =========================
from gprofiler import GProfiler
gp = GProfiler(return_dataframe=True)

def go_enrichment_for_clusters(df_clusters, cluster_col="kmeans_cluster", gene_col="gene_final",
                               organism="hsapiens", min_cluster_size=15, top_terms=5):
    enr_rows = []
    summary = []

    for cid, sub in df_clusters.groupby(cluster_col):
        genes = sub[gene_col].dropna().astype(str).unique().tolist()
        genes = [g for g in genes if g and g.lower() != "nan"]
        if len(genes) < min_cluster_size:
            continue

        try:
            res = gp.profile(
                organism=organism,
                query=genes,
                sources=["GO:BP","GO:MF","GO:CC"],
                user_threshold=0.05,
                significance_threshold_method="fdr",
            )
        except Exception as e:
            print("Cluster", cid, "GO failed:", e)
            continue

        if res is None or len(res) == 0:
            summary.append({"cluster": int(cid), "n_genes": len(genes), "n_terms_fdr05": 0})
            continue

        res = res.copy()
        res["cluster"] = int(cid)
        enr_rows.append(res)

        # gprofiler returns p_value + adjusted p_value column names may differ by version
        pcol = "p_value" if "p_value" in res.columns else ("p_value" if "p_value" in res.columns else None)
        n_sig = int((res["p_value"] <= 0.05).sum()) if "p_value" in res.columns else len(res)
        summary.append({"cluster": int(cid), "n_genes": len(genes), "n_terms_fdr05": n_sig})

    df_all = pd.concat(enr_rows, ignore_index=True) if enr_rows else pd.DataFrame()
    df_summary = pd.DataFrame(summary).sort_values("n_terms_fdr05", ascending=False)

    if len(df_summary) > 0:
        top = df_summary.head(15)
        plt.figure(figsize=(8,4))
        plt.bar(top["cluster"].astype(str), top["n_terms_fdr05"])
        plt.xlabel("Cluster")
        plt.ylabel("# GO terms (FDR<=0.05)")
        plt.title("GO enrichment strength per cluster (top 15)")
        plt.tight_layout()
        plt.savefig("go_enrichment_cluster_summary.png", dpi=300)
        plt.show()

    if len(df_all) > 0:
        df_all = df_all.sort_values(["cluster", "p_value"], ascending=[True, True])
        df_top = df_all.groupby("cluster").head(top_terms)
        df_top.to_csv("go_enrichment_top_terms_per_cluster.csv", index=False)
        print("Saved: go_enrichment_top_terms_per_cluster.csv")

    return df_all, df_summary

df_go_all, df_go_summary = go_enrichment_for_clusters(df_clusters)
display(df_go_summary.head(10))

# =========================
# 13) OPTIONAL: INDUCTIVE / OOD NODE SPLIT
# =========================
DO_INDUCTIVE = True

if DO_INDUCTIVE:
    rng = np.random.default_rng(SEED)
    all_nodes = np.arange(data_full.num_nodes)
    rng.shuffle(all_nodes)

    n_train_nodes = int(0.8 * len(all_nodes))
    train_nodes = set(all_nodes[:n_train_nodes].tolist())

    ei = data_full.edge_index.detach().cpu().numpy()
    mask = np.array([(int(u) in train_nodes and int(v) in train_nodes) for u,v in zip(ei[0], ei[1])], dtype=bool)

    edge_index_ind = torch.tensor(ei[:, mask], dtype=torch.long)
    ew_full = data_full.edge_weight.detach().cpu().numpy()
    edge_weight_ind = torch.tensor(ew_full[mask], dtype=torch.float32)

    data_ind = Data(x=data_full.x, edge_index=edge_index_ind, edge_weight=edge_weight_ind, num_nodes=data_full.num_nodes)

    split_ind = RandomLinkSplit(num_val=0.10, num_test=0.10, is_undirected=True, add_negative_train_samples=True, neg_sampling_ratio=1.0)
    tr_ind, va_ind, te_ind = split_ind(data_ind)

    tr_ind.edge_weight = edge_weight_from_dict(tr_ind.edge_index, w_dict)
    va_ind.edge_weight = edge_weight_from_dict(va_ind.edge_index, w_dict)
    te_ind.edge_weight = edge_weight_from_dict(te_ind.edge_index, w_dict)

    model_ind = LinkPredModel(tr_ind.num_node_features, hidden=128, emb_dim=64, K=10, alpha=0.1, dropout=0.5)
    model_ind, (y_ind, p_ind), _ = train_linkpred(model_ind, tr_ind, va_ind, epochs=100)

    print("[Inductive] ROC:", roc_auc_score(y_ind, p_ind), "AP:", average_precision_score(y_ind, p_ind))

import torch
from torch_geometric.nn.conv import MessagePassing

def reset_pyg_explain_mode(model):
    for m in model.modules():
        if isinstance(m, MessagePassing):
            m.explain = False
            if hasattr(m, "_edge_mask"): m._edge_mask = None
            if hasattr(m, "_loop_mask"): m._loop_mask = None
            if hasattr(m, "_apply_sigmoid"): m._apply_sigmoid = True

reset_pyg_explain_mode(model)

if torch.cuda.is_available():
    torch.cuda.empty_cache()

print("Reset PyG explain mode ✅")

# =========================
# 14) Edge explanation via PERTURBATION (robust, no GNNExplainer assertions)
# =========================
import numpy as np
import torch
import matplotlib.pyplot as plt
from torch_geometric.utils import k_hop_subgraph

u0, v0 = int(df_top.iloc[0]["u"]), int(df_top.iloc[0]["v"])
print("Explaining edge:", (u0, v0), "prob=", float(df_top.iloc[0]["prob"]))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device).eval()

# ---- build small subgraph around {u0,v0} ----
num_hops = 2     # αν είναι μεγάλο/αργό -> 1
node_pair = torch.tensor([u0, v0], dtype=torch.long)

edge_index_cpu = data_train.edge_index.detach().cpu()
x_cpu = data_train.x.detach().cpu()

edge_weight_cpu = None
if hasattr(data_train, "edge_weight") and data_train.edge_weight is not None:
    edge_weight_cpu = data_train.edge_weight.detach().cpu()

subset, ei_sub, mapping, sub_edge_mask = k_hop_subgraph(
    node_idx=node_pair,
    num_hops=num_hops,
    edge_index=edge_index_cpu,
    relabel_nodes=True
)

x_sub = x_cpu[subset]
if edge_weight_cpu is not None:
    ew_sub = edge_weight_cpu[sub_edge_mask]
else:
    ew_sub = torch.ones(ei_sub.size(1), dtype=torch.float)

# edge to explain inside subgraph
edge_label_index_sub = mapping.view(2, 1)  # [2,1]

# move to device
x_sub = x_sub.to(device)
ei_sub = ei_sub.to(device)
ew_sub = ew_sub.to(device)
edge_label_index_sub = edge_label_index_sub.to(device)

@torch.no_grad()
def link_score(edge_index, edge_weight):
    # robust to enc signature
    try:
        z = model.enc(x_sub, edge_index, edge_weight)
    except TypeError:
        z = model.enc(x_sub, edge_index)
    out = model.dec(z, edge_label_index_sub).view(-1)
    return float(out.item())

base = link_score(ei_sub, ew_sub)
print("Baseline raw score:", base)

E = ei_sub.size(1)

# ---- group reverse-direction duplicates (undirected graphs often store both) ----
# We'll remove both (a->b) and (b->a) together to be fair.
ei_cpu = ei_sub.detach().cpu()
pairs = [(int(ei_cpu[0,i]), int(ei_cpu[1,i])) for i in range(E)]
pair_to_idxs = {}
for i,(a,b) in enumerate(pairs):
    key = (min(a,b), max(a,b))
    pair_to_idxs.setdefault(key, []).append(i)

keys_all = list(pair_to_idxs.keys())

max_test = 800
if len(keys_all) > max_test:
    rng = np.random.default_rng(0)
    idx = rng.choice(len(keys_all), size=max_test, replace=False)
    keys = [keys_all[i] for i in idx]
    print(f"Subgraph unique-edges={len(keys_all)} -> testing random {len(keys)} edges for speed")
else:
    keys = keys_all

importances = []
edge_keys = []

for k in keys:
    idxs = pair_to_idxs[k]
    keep = torch.ones(E, dtype=torch.bool, device=device)
    keep[idxs] = False

    ei_drop = ei_sub[:, keep]
    ew_drop = ew_sub[keep]

    s_drop = link_score(ei_drop, ew_drop)
    imp = base - s_drop  # positive => removing edge decreases score => important
    importances.append(imp)
    edge_keys.append(k)

importances = np.array(importances, dtype=float)

# save
np.save("perturb_edge_importance.npy", importances)
print("Saved: perturb_edge_importance.npy")

# ---- plot top-20 ----
top = np.argsort(-importances)[:20]
top_imps = importances[top]
top_edges = [edge_keys[i] for i in top]

plt.figure(figsize=(8,4))
plt.bar(np.arange(len(top)), top_imps)
plt.xticks(np.arange(len(top)), [f"{a}-{b}" for (a,b) in top_edges], rotation=60, ha="right")
plt.ylabel("Importance (baseline - score_removed)")
plt.title(f"Perturbation edge importance for link ({u0},{v0}) in {num_hops}-hop subgraph")
plt.tight_layout()
plt.savefig("edge_importance_perturbation.png", dpi=300)
plt.show()

print("Saved: edge_importance_perturbation.png")

# ---- print edges back to ORIGINAL node ids ----
# map subgraph node ids -> original ids:
subset_cpu = subset.detach().cpu().numpy()
print("\nTop edges (ORIGINAL node ids, importance):")
for (a_sub, b_sub), imp in zip(top_edges, top_imps):
    a = int(subset_cpu[a_sub])
    b = int(subset_cpu[b_sub])
    print(f"  ({a}, {b})  importance={imp:.6f}")